---
layout: post
title: Deploying on-premise big data pipelines.
date: 2019-11-23 08:18:00
description: Deploying big data pipelines in an on-premise Hadoop cluster.
related_posts: false
tags: bigdata data-engineering hive airflow python
featured: true
---

# On-premise big data deployment.

## Introduction
This post is about a big data deployment we (myself and a few others in my company) did a few years ago. This was a time when only a few companies were cloud-first. Some of them hadn't ventured into the cloud at all and cloud adoption (mainly AWS at the time) was divided into part on-premise/part cloud to completely on-premise. 

This makes for some interesting choices for the tech stack. For instance, compute and storage had to be carefully managed. Permissions had to be manually handled as well.

---
## Use case and background
Prior to this exercise, most of our datawarehouse was on Postgres (again an on-premise deployment). That worked for a few years until the data volume grew from GB/week or month to TB/month or GB/day or week. We had to start exploring alternatives which were:
1. Scalable.
2. Maintainable.
3. Performant for the volume of data.
4. Aligned with the overall stack being used by the company at the time.
5. Proven to be effective via a Proof Of Concept.

After exploring many such alternatives and discussions with teams having similar use cases, we decided on the following:

### Tech stack
_Almost_ all the tools/technology used are Open Source, except the Hadoop cluster, deployed via Cloudera.
- **Compute:** Docker on Hadoop
- **Storage:** HDFS 
- **Orchestration:** Apache Airflow 
- **Analytics:** Presto on Hive with Spark compute (wrapper on Hive)
- **Language:** Python 2.7 at the time and then later 3.x
- **CI/CD:** Custom-built Airflow operators to handle various tasks.
---
## Architecture
The diagram below shows an overall architecture. We had two data sources, both of them housing fairly large amount of data - in the order of gigabytes per week to petabytes on a monthly basis.

---
## Implementation 
The implementation contains the following components:

From a Data Engineering perspective:
- Reading from the data sources.
- Transforming the data.
- Creating appropriate destination data structures (Hive tables).
- Loading the data.
- Backfills, fault tolerance of the data pipeline.
- Alerting - for workflow/job failures or upstream dependency problems.

From a Data Infrastructure perspective:
- Managing code versions.
- Handling dependencies (and its versions) required for the code to run.
- Deploying code changes.
- Permissions.

---
### Design decisions
#### Programming Language

Spark API is available in 4 langauges: Scala, Python, Java, R. Of these, 
Scala and Python were the top contenders, since Java was a new language for our tech stack
and offered little unique advantages compared to Python and Scala. 

The trade-offs between Scala and Python: 
- **Spark performance:** - although Spark offers APIS in Python and Scala, the initial versions of the 
 Python API was not as performant as the Scala API. However, later versions of the Python API caught 
 up with the Scala API's performance. 
- **Ease of use:** Did we have enough knowledge within the team to use Scala comfortably? 
  Is it worth maintaining one project in Scala whilst all the other projects have historically been in Python?
- **Maintenance:** How easy is it to maintain code? How are dependencies managed? 
- **Learning curve with Scala** - since Python was our primary language, some of us had to pick up Scala as  new programming language, which added to the complexity.

To explore the above items, we performed small Proof Of Concept exercises, such as:
- Benchmarking API performance between Scala and Python.
- Building dependencies in Scala using sbt vs. pip in Python.

#### Orchestration and workflow management
Task management, scheduling and other actions had to be managed. Apache Airflow, hosted with a Postgres backend was already available. We chose to use it. 

Airflow did not have all the operators we required. So, we ended up writing custom operators for the following:
- Python (running Python scripts)
- Hive (running queries on Hive)
- Shell operator (executing bash or other shell commands)
- Docker operator (running Docker commands)

#### Analytics
Thinking beyond the data pipelines, the decisions related to data consumption came about. 

Some options were:
- Hive -> Postgres -> Tableau/Queries/Jupyter notebooks.
- **Hive -> Presto -> Tableau(using Presto connector)/Jupyter notebooks (chosen option)**

#### File formats
Options:
- JSON.
- CSV.
- **Parquet (chosen option)**.
- ORC.

**_Reasons:_** Compression options, schema evolution, partition discovery, columnar storage and encryption and overall better performance.

### Compute
The Hadoop cluster was already available and only permissions had to be created via Hadoop Admins.

### Storage 
HDFS with a Hive metastore.

#### CI/CD
Gitlab CI.

## Consumption and analytics
Presto with Spark compute.

---
## Learning
### Performance

### Improvements

### Moving to the cloud
Equivalent services for each component.


### Maintenance
#### Docker images
#### DAGs
#### Hive metastore

---
## Conclusion


