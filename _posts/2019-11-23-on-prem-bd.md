---
layout: post
title: Deploying on-premise big data pipelines.
date: 2019-11-23 08:18:00
description: Deploying big data pipelines in an on-premise Hadoop cluster.
related_posts: false
tags: bigdata data-engineering hive airflow python
featured: true
---

# On-premise big data deployment. #

## Introduction ##
This post is about a big data deployment we (myself and a few others in my company) did a few years ago. This was a time when only a few companies were cloud-first. Some of them hadn't ventured into the cloud at all and cloud adoption (mainly AWS at the time) was divided into part on-premise/part cloud to completely on-premise. 

This makes for some interesting choices for the tech stack. For instance, compute and storage had to be carefully managed. Permissions had to be manually handled as well.

## Use case and background ##
Prior to this exercise, most of our datawarehouse was on Postgres (again an on-premise deployment). That worked for a few years until the data volume grew from GB/week or month to TB/month or GB/day or week. We had to start exploring alternatives which were:
1. Scalable.
2. Maintainable.
3. Performant for the volume of data.
4. Aligned with the overall stack being used by the company at the time.
5. Proven to be effective via a Proof Of Concept.

After exploring many such alternatives and discussions with teams having similar use cases, we decided on the following:

### Tech stack ###
_Almost_ all the tools/technology used are Open Source, except the Hadoop cluster, deployed via Cloudera.
- **Compute:** Docker on Hadoop
- **Storage:** HDFS 
- **Orchestration:** Apache Airflow 
- **Analytics:** Presto on Hive with Spark compute (wrapper on Hive)
- **Language:** Python 2.7 at the time and then later 3.x
- **CI/CD:** Custom-built Airflow operators to handle various tasks.

## Architecture ##
The diagram below shows an overall architecture. We had two data sources, both of them housing fairly large amount of data - in the order of gigabytes per week to petabytes on a monthly basis.

## Implementation ## 
The implementation contains the following components:

From a Data Engineering perspective:
- Reading from the data sources.
- Transforming the data.
- Creating appropriate destination data structures (Hive tables).
- Loading the data.
- Backfills, fault tolerance of the data pipeline.
- Alerting - for workflow/job failures or upstream dependency problems.

From a Data Infrastructure perspective:
- Managing code versions.
- Handling dependencies (and its versions) required for the code to run.
- Deploying code changes.
- Permissions.

### Design decisions ###
#### Programming Language ####

#### Workflow management ####

#### Analytics ####

#### File formats ####


### Compute ###
The Hadoop cluster was already available and only permissions had to be created via Hadoop Admins.

### Storage ###
HDFS with a Hive metastore.

### Orchestration and workflow management ###
We were using it previously for Postgres operators and managing table dependencies.
#### Operators ####
Custom-built airflow operators for Python, shell, docker, etc.

#### CI/CD ####
Gitlab CI.

## Consumption and analytics ##
Presto with Spark compute.

## Learning ##
### Performance ###

### Improvements ###

### Moving to the cloud ###
Equivalent services for each component.


### Maintenance ###
#### Docker images ####
#### DAGs #### 
#### Hive metastore ####

## Conclusion ## 


