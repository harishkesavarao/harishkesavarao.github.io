<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://harishkesavarao.github.io//feed.xml" rel="self" type="application/atom+xml"/><link href="http://harishkesavarao.github.io//" rel="alternate" type="text/html" hreflang="en"/><updated>2023-12-25T17:41:31+00:00</updated><id>http://harishkesavarao.github.io//feed.xml</id><title type="html">Harish Kesava Rao</title><subtitle>Posts about data engineering, data infrastructure and big data. </subtitle><entry><title type="html">Collection of Spark articles, tricks and more.</title><link href="http://harishkesavarao.github.io//blog/2023/spark-nuances/" rel="alternate" type="text/html" title="Collection of Spark articles, tricks and more."/><published>2023-07-29T05:18:00+00:00</published><updated>2023-07-29T05:18:00+00:00</updated><id>http://harishkesavarao.github.io//blog/2023/spark-nuances</id><content type="html" xml:base="http://harishkesavarao.github.io//blog/2023/spark-nuances/"><![CDATA[<p>Batch</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Triggers
</code></pre></div></div> <p>Streaming</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Task execution

Memory management

Configs

Diagnostics
</code></pre></div></div>]]></content><author><name></name></author><category term="bigdata"/><category term="data-engineering"/><category term="spark"/><summary type="html"><![CDATA[A collection of nuances, ideas, tricks for Spark. Includes links to blogs, articles and other docs.]]></summary></entry><entry><title type="html">Building a data lake on Microsoft Azure.</title><link href="http://harishkesavarao.github.io//blog/2023/azure-data-lake/" rel="alternate" type="text/html" title="Building a data lake on Microsoft Azure."/><published>2023-03-01T05:18:00+00:00</published><updated>2023-03-01T05:18:00+00:00</updated><id>http://harishkesavarao.github.io//blog/2023/azure-data-lake</id><content type="html" xml:base="http://harishkesavarao.github.io//blog/2023/azure-data-lake/"><![CDATA[<h1 id="introduction">Introduction</h1> <h2 id="what-is-a-data-lake">What is a Data Lake?</h2> <h1 id="use-cases">Use cases</h1> <h1 id="architecture">Architecture</h1> <h1 id="data-sources">Data sources</h1> <h2 id="storage-account">Storage Account</h2> <h2 id="synapse">Synapse</h2> <h2 id="eventhubs">Eventhubs</h2> <h2 id="event-queue">Event Queue</h2> <h1 id="destination">Destination</h1> <h1 id="design-and-implementation">Design and Implementation</h1> <h2 id="compute">Compute</h2> <h2 id="storage">Storage</h2> <h3 id="storage-account-blob">Storage account (blob)</h3> <h2 id="analytics">Analytics</h2> <h3 id="tableau">Tableau</h3> <h2 id="security-and-permissions">Security and permissions</h2> <h2 id="costs-and-scaling-updown">Costs and Scaling up/down</h2> <h2 id="alerting-and-monitoring">Alerting and Monitoring</h2> <h2 id="performance-tuning">Performance Tuning</h2> <h1 id="conclusion">Conclusion</h1>]]></content><author><name></name></author><category term="bigdata"/><category term="azure"/><category term="data-engineering"/><category term="scala"/><category term="python"/><summary type="html"><![CDATA[Building and deploying a data lake on Azure infrastructure.]]></summary></entry><entry><title type="html">Building a data lake on Amazon Web Services.</title><link href="http://harishkesavarao.github.io//blog/2021/aws-data-lake/" rel="alternate" type="text/html" title="Building a data lake on Amazon Web Services."/><published>2021-06-01T05:18:00+00:00</published><updated>2021-06-01T05:18:00+00:00</updated><id>http://harishkesavarao.github.io//blog/2021/aws-data-lake</id><content type="html" xml:base="http://harishkesavarao.github.io//blog/2021/aws-data-lake/"><![CDATA[<h1 id="introduction">Introduction</h1> <h2 id="pre-requisite-reading">Pre-requisite reading</h2> <p>In order to fully understand or follow along with the article, I recommend reading some of the documents, articles and other links I have included in this section. If you have already worked on the AWS services I have listed below, you can skip this section.</p> <ul> <li><a href="https://aws.amazon.com/getting-started/cloud-essentials/">AWS Cloud essentials</a>.</li> <li><a href="https://docs.aws.amazon.com/whitepapers/latest/aws-overview/amazon-web-services-cloud-platform.html?pg=cloudessentials">AWS Services by category</a>.</li> <li><a href="https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html">AWS Organizations</a>.</li> <li><a href="https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts.html">AWS Accounts</a>.</li> <li><a href="https://docs.aws.amazon.com/whitepapers/latest/aws-overview/accessing-aws-services.html">AWS Management Console</a>.</li> <li><a href="https://docs.aws.amazon.com/whitepapers/latest/aws-overview/compute-services.html">AWS Compute</a>.</li> <li><a href="https://docs.aws.amazon.com/whitepapers/latest/aws-overview/storage-services.html">AWS Storage</a>.</li> <li><a href="https://docs.aws.amazon.com/whitepapers/latest/aws-overview/analytics.html">AWS Analytics</a>.</li> <li><a href="https://docs.aws.amazon.com/whitepapers/latest/aws-overview/containers.html">AWS Containers</a>.</li> <li><a href="https://docs.aws.amazon.com/whitepapers/latest/aws-overview/application-integration.html">AWS Application Integration</a>.</li> <li><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html">AWS IAM</a>.</li> <li><a href="https://aws.amazon.com/what-is/data-lake/">What is a Data Lake</a>.</li> </ul> <h3 id="advanced-reading">Advanced reading</h3> <ul> <li><a href="https://docs.aws.amazon.com/whitepapers/latest/aws-overview/networking-services.html">AWS Networking Services</a>.</li> <li><a href="https://docs.aws.amazon.com/whitepapers/latest/aws-overview/management-governance.html">AWS Management and Governance</a>.</li> <li><a href="https://docs.aws.amazon.com/whitepapers/latest/aws-overview/developer-tools.html">AWS Developer Tools</a>.</li> <li><a href="https://docs.aws.amazon.com/whitepapers/latest/aws-overview/aws-cost-management.html">Cost management in AWS</a>.</li> <li><a href="https://docs.aws.amazon.com/whitepapers/latest/aws-overview/database.html">AWS Databases</a>.</li> </ul> <h3 id="infrastructure-as-code">Infrastructure as Code</h3> <p>All of the AWS resources discussed can be created manually via the AWS Management Console.</p> <p>In a production deployment, that is seldom the case. We typically use an <a href="https://developer.hashicorp.com/terraform/tutorials/aws-get-started/infrastructure-as-code">Infrastructure as Code</a> deployment to manage resources. There are a few IaC options, Terraform and AWS CDK being the most popular.</p> <p>I have worked on both and there are pros and cons with both.</p> <p>AWS CDK allows many commonly used languages to define resources - such as Python, Java, TypeScript, Go, JavaScript etc. AWS CDK uses <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-concepts.html">AWS CloudFormation</a> behind the scenes to deploy resources. It also comes with the same limitations as <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cloudformation-limits.html">AWS CloudFormation</a>.</p> <p>Terraform uses its own <a href="https://developer.hashicorp.com/terraform/language">configuration language</a> across different clouds, which is convenient if you have a multi-cloud infrastructure. It might take some time to learn Terraform’s configuration language, but isn’t that difficult as you begin creating and deploying different resources to AWS (or to any other cloud for that matter). Terraform uses declarative syntax as opposed to other common programming langauges.</p> <p>To illustrate resource creation in this article, I will use Terraform examples for resource creation.</p> <h2 id="what-is-a-data-lake">What is a Data Lake?</h2> <p><a href="#pre-requisite-reading">Reading for this section: What is a Data Lake</a></p> <blockquote> <p>A data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. You can store your data as-is, without having to first structure the data, and run different types of analytics—from dashboards and visualizations to big data processing, real-time analytics, and machine learning to guide better decisions.</p> </blockquote> <h1 id="use-cases">Use cases</h1> <p>It is cost effective and performant to build a Data Lake when the data volume is over a certain threshold. The threshold is usually in the tens of GBs of data per day and continues to accumulate over time. If the data volume is less than that, it would be quicker and cheaper to build a traditional data warehouse or other solutions.</p> <p>As we saw above, a Data Lake is a centralized repository which stores all of the structured and unstructured data. This means that this data is available to anyone in an organization (with the relevant permissions). This could be - Data Analysts, Data Scientists, Data Engineers, Business Analysts, Product Managers, Finance or any other function. This offers a single source of truth of the data and each sub-function or department within an organization can choose to use the data as they see fit, with their own tooling for data access, analytics and visualization.</p> <h1 id="architecture">Architecture</h1> <p>First, we will try to see the different pieces of a typical big data flow. Then, we can explore the different options available in AWS to accommodate those pieces. This will help us arrive at our actual architecture diagram which will more closely represent our actual implementation.</p> <p><code class="language-plaintext highlighter-rouge">TODO: Data Flow Diagram</code></p> <h1 id="data-sources">Data sources</h1> <p>Typically, data sources come from within AWS itself. In rare exceptions, the data comes from outside the cloud, or from another cloud provider. We will discuss both scenarios.</p> <h2 id="aws-s3">AWS S3</h2> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html">AWS S3 (Simple Storage Service)</a> is one of the most common use cases of consuming data in AWS. You can read about the basics (buckets, etc.) of S3 from the documentation link.</p> <h3 id="cross-account-storage">Cross-account storage</h3> <p><a href="#pre-requisite-reading">Reading for this section: AWS Organizations and AWS Accounts</a> Sometimes, the S3 bucket containing the data of interest may not reside in the same AWS account from which we are reading it. To begin reading from such external accounts, the required permissions need to be in place.</p> <ol> <li>Role in destination account.</li> <li>Role in source/data source account.</li> <li>Trust policy in source/data source account.</li> <li>Policy related to the source/data source account.</li> </ol> <h3 id="storage-lifecycle-and-other-related-items">Storage lifecycle and other related items</h3> <h3 id="security">Security</h3> <h3 id="encryption">Encryption</h3> <h3 id="cost">Cost</h3> <h4 id="multi-region-data">Multi-region data</h4> <h2 id="redshift">Redshift</h2> <h3 id="analytics">Analytics</h3> <h3 id="performance-and-scaling">Performance and scaling</h3> <h2 id="kinesis">Kinesis</h2> <h2 id="cloudwatch">Cloudwatch</h2> <h2 id="external-data-sources">External data sources</h2> <h3 id="from-an-on-premise-source">From an on-premise source</h3> <h3 id="from-another-cloud-provider">From another cloud provider</h3> <h3 id="others">Others</h3> <p>Flat files Spreadsheets SaaS systems/APIs</p> <h1 id="destination">Destination</h1> <h1 id="design-decisions-trade-offs">Design decisions, trade-offs</h1> <h2 id="compute">Compute</h2> <h3 id="ec2">EC2</h3> <h3 id="emr">EMR</h3> <h3 id="sagemaker">SageMaker</h3> <h2 id="storage">Storage</h2> <h3 id="s3">S3</h3> <h1 id="etl">ETL</h1> <h2 id="security-and-permissions">Security and permissions</h2> <h2 id="batch-vs-streaming">Batch vs. Streaming</h2> <h1 id="costs-and-scaling-updown">Costs and Scaling up/down</h1> <h2 id="compute-1">Compute</h2> <h2 id="storage-1">Storage</h2> <h2 id="in-flight-data">In-flight data</h2> <h1 id="alerting-and-monitoring">Alerting and Monitoring</h1> <h2 id="cloudwatch-1">Cloudwatch</h2> <h2 id="eventbridge">Eventbridge</h2> <h2 id="performance-tuning">Performance Tuning</h2> <h1 id="analytics-1">Analytics</h1> <h2 id="cataloging-accuracy-and-governance">Cataloging, accuracy and governance</h2> <h2 id="tooling">Tooling</h2> <h3 id="quicksight">Quicksight</h3> <h3 id="sagemaker-1">Sagemaker</h3> <h3 id="tableau">Tableau</h3> <h1 id="reference-architecture-diagram">Reference Architecture Diagram</h1> <p><code class="language-plaintext highlighter-rouge">TODO: Architecture Diagram</code></p> <h2 id="devops">DevOps</h2> <h3 id="terraform">Terraform</h3> <h3 id="aws-cdk">AWS CDK</h3> <h3 id="codeapplication">Code/Application</h3> <h1 id="conclusion">Conclusion</h1>]]></content><author><name></name></author><category term="bigdata"/><category term="aws"/><category term="data-engineering"/><category term="scala"/><summary type="html"><![CDATA[Building and deploying a data lake on AWS infrastructure.]]></summary></entry><entry><title type="html">Deploying on-premise big data pipelines.</title><link href="http://harishkesavarao.github.io//blog/2019/on-prem-bd/" rel="alternate" type="text/html" title="Deploying on-premise big data pipelines."/><published>2019-11-23T08:18:00+00:00</published><updated>2019-11-23T08:18:00+00:00</updated><id>http://harishkesavarao.github.io//blog/2019/on-prem-bd</id><content type="html" xml:base="http://harishkesavarao.github.io//blog/2019/on-prem-bd/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>This post is about a big data deployment we (myself and a few others in my company) did a few years ago. This was a time when only a few companies were cloud-first. Some of them hadn’t ventured into the cloud at all and cloud adoption (mainly AWS at the time) was divided into part on-premise/part cloud to completely on-premise.</p> <p>This makes for some interesting choices for the tech stack. For instance, compute and storage had to be carefully managed. Permissions had to be manually handled as well.</p> <h1 id="use-case-and-background">Use case and background</h1> <p>Prior to this exercise, most of our datawarehouse was on Postgres (again an on-premise deployment). That worked for a few years until the data volume grew from GB/week or month to TB/month or GB/day or week. We had to start exploring alternatives which were:</p> <ol> <li>Scalable.</li> <li>Maintainable.</li> <li>Performant for the volume of data.</li> <li>Aligned with the overall stack being used by the company at the time.</li> <li>Proven to be effective via a Proof Of Concept.</li> </ol> <p>After exploring many such alternatives and discussions with teams having similar use cases, we decided on the following:</p> <h1 id="tech-stack">Tech stack</h1> <p><em>Almost</em> all the tools/technology used are Open Source, except the Hadoop cluster, deployed via Cloudera.</p> <table> <thead> <tr> <th>Area</th> <th>Tools/technology</th> </tr> </thead> <tbody> <tr> <td>Compute</td> <td>Hadoop</td> </tr> <tr> <td>Storage</td> <td>HDFS</td> </tr> <tr> <td>Orchestration, workflow management</td> <td>Apache Airflow</td> </tr> <tr> <td>Containerization</td> <td>Docker (published to GitLab container registry)</td> </tr> <tr> <td>Programming Language</td> <td>Python 2.7 at the time and later 3.x, PySpark</td> </tr> <tr> <td>Analytics/data presentation layer</td> <td>Presto on Apache Hive with Spark compute (wrapper on Apache Hive)</td> </tr> <tr> <td>CI/CD</td> <td>Custom-built Airflow operators</td> </tr> </tbody> </table> <h1 id="architecture">Architecture</h1> <p>The diagram below shows an overall architecture. We had two data sources, both of them housing fairly large amount of data - in the order of gigabytes per week to petabytes on a monthly basis.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/on-prem-bd-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/on-prem-bd-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/on-prem-bd-1400.webp"/> <img src="/assets/img/on-prem-bd.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="implementation">Implementation</h1> <p>The implementation contains the following components:</p> <p>From a <strong>Data Engineering</strong> perspective:</p> <ul> <li>Reading from the data sources.</li> <li>Transforming the data.</li> <li>Creating appropriate destination data structures (Apache Hive tables).</li> <li>Loading the data.</li> <li>Backfills, fault tolerance of the data pipeline.</li> <li>Alerting - for workflow/job failures or upstream dependency problems.</li> </ul> <p>From a <strong>Data Infrastructure</strong> perspective:</p> <ul> <li>Managing code versions.</li> <li>Handling dependencies (and its versions) required for the code to run.</li> <li>Deploying code changes.</li> <li>Permissions.</li> </ul> <h1 id="design-decisions">Design decisions</h1> <h2 id="programming-language">Programming Language</h2> <blockquote> <p>Python/PySpark.</p> </blockquote> <p>Spark API is available in 4 langauges: Scala, Python, Java, R. Of these, Scala and Python were the top contenders, since Java was a new language for our tech stack and offered little unique advantages compared to Python and Scala.</p> <p>The trade-offs between Scala and Python:</p> <ul> <li><strong>Spark performance:</strong> - although Spark offers APIS in Python and Scala, the initial versions of the Python API was not as performant as the Scala API. However, later versions of the Python API caught up with the Scala API’s performance.</li> <li><strong>Ease of use:</strong> Did we have enough knowledge within the team to use Scala comfortably? Is it worth maintaining one project in Scala whilst all the other projects have historically been in Python?</li> <li><strong>Maintenance:</strong> How easy is it to maintain code? How are dependencies managed?</li> <li><strong>Learning curve with Scala</strong> - since Python was our primary language, some of us had to pick up Scala as new programming language, which added to the complexity.</li> </ul> <p>To explore the above items, we performed small Proof Of Concept exercises, such as:</p> <ul> <li>Benchmarking API performance between Scala and Python.</li> <li>Building dependencies in Scala using sbt vs. pip in Python.</li> </ul> <h2 id="orchestration-and-workflow-management">Orchestration and workflow management</h2> <blockquote> <p>Apache Airflow</p> </blockquote> <p>Task management, scheduling and other actions had to be managed. Apache Airflow, hosted with a Postgres backend was already available. We chose to use it.</p> <p>Airflow did not have all the operators we required. So, we ended up writing custom operators for the following:</p> <ul> <li>Python (running Python scripts)</li> <li>Apache Hive (running queries on Apache Hive)</li> <li>Shell operator (executing bash or other shell commands)</li> <li>Docker operator (running Docker commands)</li> </ul> <h2 id="analytics">Analytics</h2> <blockquote> <p>Apache Hive/Presto</p> </blockquote> <p>Thinking beyond the data pipelines, the decisions related to data consumption came about.</p> <p>Some options were:</p> <ul> <li>Apache Hive -&gt; Postgres -&gt; Tableau/Queries/Jupyter notebooks.</li> <li>Apache Hive -&gt; Presto -&gt; Tableau(using Presto connector)/Jupyter notebooks.</li> </ul> <h2 id="file-formats">File formats</h2> <blockquote> <p>Parquet.</p> </blockquote> <p>Options:</p> <ul> <li>JSON.</li> <li>CSV.</li> <li>Parquet.</li> <li>ORC.</li> </ul> <p><strong><em>Reasons:</em></strong> Compression options, schema evolution, partition discovery, columnar storage and encryption and overall better performance.</p> <h2 id="compute">Compute</h2> <p>A shared Hadoop cluster was already available and only permissions had to be created to access the cluster.</p> <h2 id="storage">Storage</h2> <p>Straightforward HDFS storage with a Apache Hive metastore. We created a new Apache Hive metastore for our project since the entire storage was shared.</p> <h2 id="cicd">CI/CD</h2> <p>We used GitLab for the following:</p> <ol> <li>Code versions.</li> <li>Publishing docker image versions to GitLab container registry.</li> <li>Running pytest checks.</li> </ol> <h2 id="consumption-and-analytics">Consumption and analytics</h2> <p>Now that we have discussed the ingestion pipeline, the next step is making the datasets available for users.</p> <p>Since we are dealing with large data volumes, querying Apache Hive directly wasn’t scalable (via Presto). We used Presto on Spark.</p> <p>Analytics was done on a Jupyter notebook, so the query results were used to build visualizations.</p> <p>Analytics teams built their own DAGs using the datasets we created.</p> <h1 id="learning">Learning</h1> <h2 id="performance">Performance</h2> <p>The Hadoop cluster was performant to ingest data at the rate we expected. The query response time was dependent on the partitioning of the Apache Hive table.</p> <h2 id="improvements">Improvements</h2> <h3 id="some-future-improvements">Some future improvements:</h3> <ul> <li>Managing Python dependencies/packaging better using npm.</li> <li>Creating leaner docker images to improve deployment times.</li> <li>Moving away from Apache Hive (see the section on Moving to the cloud).</li> </ul> <h2 id="moving-to-the-cloud">Moving to the cloud</h2> <p>When the migration to the cloud was started, the following changes were made to the architecture:</p> <table> <thead> <tr> <th>Type</th> <th>On-premise</th> <th>Cloud</th> </tr> </thead> <tbody> <tr> <td>Query Engine</td> <td>Hive</td> <td>Snowflake</td> </tr> <tr> <td>Storage</td> <td>HDFS</td> <td>AWS S3</td> </tr> <tr> <td>Compute</td> <td>Hadoop</td> <td>AWS EMR, EC2</td> </tr> <tr> <td>CI/CD</td> <td>Gitlab CI</td> <td>AWS Codebuild</td> </tr> <tr> <td>Docker registry</td> <td>GitLab container registry</td> <td>AWS ECR</td> </tr> <tr> <td>Orchestration, workflow management</td> <td>Apache Airflow</td> <td>Amazon Managed Workflows for Apache Airflow (MWAA)</td> </tr> <tr> <td>Permissions</td> <td>Manual</td> <td>AWS IAM</td> </tr> </tbody> </table> <hr/> <h1 id="conclusion">Conclusion</h1> <p>Overall, the project helped us foray into the big data area and keep up with current and future data needs of the organization.</p> <p>The cloud allowed us to scale and automate many of the steps for development, deployment and maintenance.</p>]]></content><author><name></name></author><category term="bigdata"/><category term="data-engineering"/><category term="hive"/><category term="airflow"/><category term="python"/><summary type="html"><![CDATA[Deploying big data pipelines in an on-premise Hadoop cluster.]]></summary></entry><entry><title type="html">Collection of articles, tricks for ETL.</title><link href="http://harishkesavarao.github.io//blog/2018/etl-nuances/" rel="alternate" type="text/html" title="Collection of articles, tricks for ETL."/><published>2018-07-29T05:18:00+00:00</published><updated>2018-07-29T05:18:00+00:00</updated><id>http://harishkesavarao.github.io//blog/2018/etl-nuances</id><content type="html" xml:base="http://harishkesavarao.github.io//blog/2018/etl-nuances/"><![CDATA[<p>Dimensional Modeling</p> <p>ETL approaches</p>]]></content><author><name></name></author><category term="etl"/><category term="data-engineering"/><summary type="html"><![CDATA[A collection of nuances, ideas, tricks for ETL. Includes links to blogs, articles and other docs.]]></summary></entry></feed>