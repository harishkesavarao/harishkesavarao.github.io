<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://harishkesavarao.github.io//feed.xml" rel="self" type="application/atom+xml"/><link href="http://harishkesavarao.github.io//" rel="alternate" type="text/html" hreflang="en"/><updated>2023-12-21T05:48:52+00:00</updated><id>http://harishkesavarao.github.io//feed.xml</id><title type="html">Harish Kesava Rao</title><subtitle>Posts about data engineering, data infrastructure and big data. </subtitle><entry><title type="html">Building a data lake on Microsoft Azure.</title><link href="http://harishkesavarao.github.io//blog/2023/azure-data-lake/" rel="alternate" type="text/html" title="Building a data lake on Microsoft Azure."/><published>2023-03-01T05:18:00+00:00</published><updated>2023-03-01T05:18:00+00:00</updated><id>http://harishkesavarao.github.io//blog/2023/azure-data-lake</id><content type="html" xml:base="http://harishkesavarao.github.io//blog/2023/azure-data-lake/"><![CDATA[<h1 id="introduction">Introduction</h1> <h2 id="what-is-a-data-lake">What is a Data Lake?</h2> <h1 id="use-cases">Use cases</h1> <h1 id="architecture">Architecture</h1> <h1 id="data-sources">Data sources</h1> <h2 id="storage-account">Storage Account</h2> <h2 id="synapse">Synapse</h2> <h2 id="eventhubs">Eventhubs</h2> <h2 id="event-queue">Event Queue</h2> <h1 id="destination">Destination</h1> <h1 id="design-and-implementation">Design and Implementation</h1> <h2 id="compute">Compute</h2> <h2 id="storage">Storage</h2> <h3 id="storage-account-blob">Storage account (blob)</h3> <h2 id="analytics">Analytics</h2> <h3 id="tableau">Tableau</h3> <h2 id="security-and-permissions">Security and permissions</h2> <h2 id="costs-and-scaling-updown">Costs and Scaling up/down</h2> <h2 id="alerting-and-monitoring">Alerting and Monitoring</h2> <h2 id="performance-tuning">Performance Tuning</h2> <h1 id="conclusion">Conclusion</h1>]]></content><author><name></name></author><category term="bigdata"/><category term="azure"/><category term="data-engineering"/><category term="scala"/><category term="python"/><summary type="html"><![CDATA[Building and deploying a data lake on Azure infrastructure.]]></summary></entry><entry><title type="html">Building a data lake on Amazon Web Services.</title><link href="http://harishkesavarao.github.io//blog/2021/aws-data-lake/" rel="alternate" type="text/html" title="Building a data lake on Amazon Web Services."/><published>2021-06-01T05:18:00+00:00</published><updated>2021-06-01T05:18:00+00:00</updated><id>http://harishkesavarao.github.io//blog/2021/aws-data-lake</id><content type="html" xml:base="http://harishkesavarao.github.io//blog/2021/aws-data-lake/"><![CDATA[<h1 id="introduction">Introduction</h1> <h2 id="what-is-a-data-lake">What is a Data Lake?</h2> <blockquote> <p>A data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. You can store your data as-is, without having to first structure the data, and run different types of analytics—from dashboards and visualizations to big data processing, real-time analytics, and machine learning to guide better decisions.</p> </blockquote> <p>The AWS website has a good <a href="https://aws.amazon.com/what-is/data-lake/">article</a> which explains more about a Data Lake and how it compares to a traditional data warehouse,</p> <h1 id="use-cases">Use cases</h1> <p>It is cost effective and performant to build a Data Lake when the data volume is over a certain threshold. The threshold is usually in the tens of GBs of data per day and continues to accumulate over time. If the data volume is less than that, it would be quicker and cheaper to build a traditional data warehouse or other solutions.</p> <p>As we saw above, a Data Lake is a centralized repository which stores all of the structured and unstructured data. This means that this data is available to anyone in an organization (with the relevant permissions). This could be - Data Analysts, Data Scientists, Data Engineers, Business Analysts, Product Managers, Finance or any other function. This offers a single source of truth of the data and each sub-function or department within an organization can choose to use the data as they see fit, with their own tooling for data access, analytics and visualization.</p> <h1 id="architecture">Architecture</h1> <p>First, we will try to see the different pieces of a typical big data flow. Then, we can explore the different options available in AWS to accomodate those pieces. This will help us arrive at our actual architecture diagram which will more closely represent our actual implementation.</p> <p><code class="language-plaintext highlighter-rouge">TODO: Data Flow Diagram</code></p> <h1 id="data-sources">Data sources</h1> <p>Typically, data sources come from within AWS itself. In rare exceptions, the data comes from outside the cloud, or from another cloud provider. We will discuss both scenarios.</p> <h2 id="s3">S3</h2> <h3 id="storage">Storage</h3> <h3 id="retention">Retention</h3> <h3 id="versioning">Versioning</h3> <h3 id="security">Security</h3> <h3 id="encryption">Encryption</h3> <h2 id="redshift">Redshift</h2> <h3 id="analytics">Analytics</h3> <h3 id="performance-and-scaling">Performance and scaling</h3> <h2 id="kinesis">Kinesis</h2> <h2 id="cloudwatch">Cloudwatch</h2> <h1 id="destination">Destination</h1> <h1 id="design-and-implementation">Design and Implementation</h1> <h2 id="compute">Compute</h2> <h3 id="ec2">EC2</h3> <h3 id="emr">EMR</h3> <h3 id="sagemaker">SageMaker</h3> <h2 id="storage-1">Storage</h2> <h3 id="s3-1">S3</h3> <h2 id="security-and-permissions">Security and permissions</h2> <h2 id="costs-and-scaling-updown">Costs and Scaling up/down</h2> <h2 id="alerting-and-monitoring">Alerting and Monitoring</h2> <h2 id="cloudwatch-1">Cloudwatch</h2> <h2 id="eventbridge">Eventbridge</h2> <h2 id="performance-tuning">Performance Tuning</h2> <h2 id="cataloging-accuracy-and-governance">Cataloging, accuracy and governance</h2> <h2 id="analytics-1">Analytics</h2> <h3 id="quicksight">Quicksight</h3> <h3 id="sagemaker-1">Sagemaker</h3> <h3 id="tableau">Tableau</h3> <h1 id="reference-architecture-diagram">Reference Architecture Diagram</h1> <p><code class="language-plaintext highlighter-rouge">TODO: Architecture Diagram</code></p> <h2 id="devops">DevOps</h2> <h3 id="terraform">Terraform</h3> <h3 id="aws-cdk">AWS CDK</h3> <h3 id="codeapplication">Code/Application</h3> <h1 id="conclusion">Conclusion</h1>]]></content><author><name></name></author><category term="bigdata"/><category term="aws"/><category term="data-engineering"/><category term="scala"/><summary type="html"><![CDATA[Building and deploying a data lake on AWS infrastructure.]]></summary></entry><entry><title type="html">Deploying on-premise big data pipelines.</title><link href="http://harishkesavarao.github.io//blog/2019/on-prem-bd/" rel="alternate" type="text/html" title="Deploying on-premise big data pipelines."/><published>2019-11-23T08:18:00+00:00</published><updated>2019-11-23T08:18:00+00:00</updated><id>http://harishkesavarao.github.io//blog/2019/on-prem-bd</id><content type="html" xml:base="http://harishkesavarao.github.io//blog/2019/on-prem-bd/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>This post is about a big data deployment we (myself and a few others in my company) did a few years ago. This was a time when only a few companies were cloud-first. Some of them hadn’t ventured into the cloud at all and cloud adoption (mainly AWS at the time) was divided into part on-premise/part cloud to completely on-premise.</p> <p>This makes for some interesting choices for the tech stack. For instance, compute and storage had to be carefully managed. Permissions had to be manually handled as well.</p> <h1 id="use-case-and-background">Use case and background</h1> <p>Prior to this exercise, most of our datawarehouse was on Postgres (again an on-premise deployment). That worked for a few years until the data volume grew from GB/week or month to TB/month or GB/day or week. We had to start exploring alternatives which were:</p> <ol> <li>Scalable.</li> <li>Maintainable.</li> <li>Performant for the volume of data.</li> <li>Aligned with the overall stack being used by the company at the time.</li> <li>Proven to be effective via a Proof Of Concept.</li> </ol> <p>After exploring many such alternatives and discussions with teams having similar use cases, we decided on the following:</p> <h1 id="tech-stack">Tech stack</h1> <p><em>Almost</em> all the tools/technology used are Open Source, except the Hadoop cluster, deployed via Cloudera.</p> <table> <thead> <tr> <th>Area</th> <th>Tools/technology</th> </tr> </thead> <tbody> <tr> <td>Compute</td> <td>Hadoop</td> </tr> <tr> <td>Storage</td> <td>HDFS</td> </tr> <tr> <td>Orchestration, workflow management</td> <td>Apache Airflow</td> </tr> <tr> <td>Containerization</td> <td>Docker (published to GitLab container registry)</td> </tr> <tr> <td>Programming Language</td> <td>Python 2.7 at the time and later 3.x, PySpark</td> </tr> <tr> <td>Analytics/data presentation layer</td> <td>Presto on Apache Hive with Spark compute (wrapper on Apache Hive)</td> </tr> <tr> <td>CI/CD</td> <td>Custom-built Airflow operators</td> </tr> </tbody> </table> <h1 id="architecture">Architecture</h1> <p>The diagram below shows an overall architecture. We had two data sources, both of them housing fairly large amount of data - in the order of gigabytes per week to petabytes on a monthly basis.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/on-prem-bd-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/on-prem-bd-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/on-prem-bd-1400.webp"/> <img src="/assets/img/on-prem-bd.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="implementation">Implementation</h1> <p>The implementation contains the following components:</p> <p>From a <strong>Data Engineering</strong> perspective:</p> <ul> <li>Reading from the data sources.</li> <li>Transforming the data.</li> <li>Creating appropriate destination data structures (Apache Hive tables).</li> <li>Loading the data.</li> <li>Backfills, fault tolerance of the data pipeline.</li> <li>Alerting - for workflow/job failures or upstream dependency problems.</li> </ul> <p>From a <strong>Data Infrastructure</strong> perspective:</p> <ul> <li>Managing code versions.</li> <li>Handling dependencies (and its versions) required for the code to run.</li> <li>Deploying code changes.</li> <li>Permissions.</li> </ul> <h1 id="design-decisions">Design decisions</h1> <h2 id="programming-language">Programming Language</h2> <blockquote> <p>Python/PySpark.</p> </blockquote> <p>Spark API is available in 4 langauges: Scala, Python, Java, R. Of these, Scala and Python were the top contenders, since Java was a new language for our tech stack and offered little unique advantages compared to Python and Scala.</p> <p>The trade-offs between Scala and Python:</p> <ul> <li><strong>Spark performance:</strong> - although Spark offers APIS in Python and Scala, the initial versions of the Python API was not as performant as the Scala API. However, later versions of the Python API caught up with the Scala API’s performance.</li> <li><strong>Ease of use:</strong> Did we have enough knowledge within the team to use Scala comfortably? Is it worth maintaining one project in Scala whilst all the other projects have historically been in Python?</li> <li><strong>Maintenance:</strong> How easy is it to maintain code? How are dependencies managed?</li> <li><strong>Learning curve with Scala</strong> - since Python was our primary language, some of us had to pick up Scala as new programming language, which added to the complexity.</li> </ul> <p>To explore the above items, we performed small Proof Of Concept exercises, such as:</p> <ul> <li>Benchmarking API performance between Scala and Python.</li> <li>Building dependencies in Scala using sbt vs. pip in Python.</li> </ul> <h2 id="orchestration-and-workflow-management">Orchestration and workflow management</h2> <blockquote> <p>Apache Airflow</p> </blockquote> <p>Task management, scheduling and other actions had to be managed. Apache Airflow, hosted with a Postgres backend was already available. We chose to use it.</p> <p>Airflow did not have all the operators we required. So, we ended up writing custom operators for the following:</p> <ul> <li>Python (running Python scripts)</li> <li>Apache Hive (running queries on Apache Hive)</li> <li>Shell operator (executing bash or other shell commands)</li> <li>Docker operator (running Docker commands)</li> </ul> <h2 id="analytics">Analytics</h2> <blockquote> <p>Apache Hive/Presto</p> </blockquote> <p>Thinking beyond the data pipelines, the decisions related to data consumption came about.</p> <p>Some options were:</p> <ul> <li>Apache Hive -&gt; Postgres -&gt; Tableau/Queries/Jupyter notebooks.</li> <li>Apache Hive -&gt; Presto -&gt; Tableau(using Presto connector)/Jupyter notebooks.</li> </ul> <h2 id="file-formats">File formats</h2> <blockquote> <p>Parquet.</p> </blockquote> <p>Options:</p> <ul> <li>JSON.</li> <li>CSV.</li> <li>Parquet.</li> <li>ORC.</li> </ul> <p><strong><em>Reasons:</em></strong> Compression options, schema evolution, partition discovery, columnar storage and encryption and overall better performance.</p> <h2 id="compute">Compute</h2> <p>A shared Hadoop cluster was already available and only permissions had to be created to access the cluster.</p> <h2 id="storage">Storage</h2> <p>Straightforward HDFS storage with a Apache Hive metastore. We created a new Apache Hive metastore for our project since the entire storage was shared.</p> <h2 id="cicd">CI/CD</h2> <p>We used GitLab for the following:</p> <ol> <li>Code versions.</li> <li>Publishing docker image versions to GitLab container registry.</li> <li>Running pytest checks.</li> </ol> <h2 id="consumption-and-analytics">Consumption and analytics</h2> <p>Now that we have discussed the ingestion pipeline, the next step is making the datasets available for users.</p> <p>Since we are dealing with large data volumes, querying Apache Hive directly wasn’t scalable (via Presto). We used Presto on Spark.</p> <p>Analytics was done on a Jupyter notebook, so the query results were used to build visualizations.</p> <p>Analytics teams built their own DAGs using the datasets we created.</p> <h1 id="learning">Learning</h1> <h2 id="performance">Performance</h2> <p>The Hadoop cluster was performant to ingest data at the rate we expected. The query response time was dependent on the partitioning of the Apache Hive table.</p> <h2 id="improvements">Improvements</h2> <h3 id="some-future-improvements">Some future improvements:</h3> <ul> <li>Managing Python dependencies/packaging better using npm.</li> <li>Creating leaner docker images to improve deployment times.</li> <li>Moving away from Apache Hive (see the section on Moving to the cloud).</li> </ul> <h2 id="moving-to-the-cloud">Moving to the cloud</h2> <p>When the migration to the cloud was started, the following changes were made to the architecture:</p> <table> <thead> <tr> <th>Type</th> <th>On-premise</th> <th>Cloud</th> </tr> </thead> <tbody> <tr> <td>Query Engine</td> <td>Hive</td> <td>Snowflake</td> </tr> <tr> <td>Storage</td> <td>HDFS</td> <td>AWS S3</td> </tr> <tr> <td>Compute</td> <td>Hadoop</td> <td>AWS EMR, EC2</td> </tr> <tr> <td>CI/CD</td> <td>Gitlab CI</td> <td>AWS Codebuild</td> </tr> <tr> <td>Docker registry</td> <td>GitLab container registry</td> <td>AWS ECR</td> </tr> <tr> <td>Orchestration, workflow management</td> <td>Apache Airflow</td> <td>Amazon Managed Workflows for Apache Airflow (MWAA)</td> </tr> <tr> <td>Permissions</td> <td>Manual</td> <td>AWS IAM</td> </tr> </tbody> </table> <hr/> <h1 id="conclusion">Conclusion</h1> <p>Overall, the project helped us foray into the big data area and keep up with current and future data needs of the organization.</p> <p>The cloud allowed us to scale and automate many of the steps for development, deployment and maintenance.</p>]]></content><author><name></name></author><category term="bigdata"/><category term="data-engineering"/><category term="hive"/><category term="airflow"/><category term="python"/><summary type="html"><![CDATA[Deploying big data pipelines in an on-premise Hadoop cluster.]]></summary></entry></feed>